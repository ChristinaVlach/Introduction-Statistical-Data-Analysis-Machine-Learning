{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLASSIFICATION ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "The goal of classification analysis is to predict the category or class to which a new observation belongs, based on a set of input variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set of observed data points $X = {x_0, x_1, ..., x_n}$ with corresponding class labels $Y = { y_0, y_1, ..., y_n}$, the objective is to find a function $y = f(x, \\theta)$ that maps input variables $x$ to class labels $y$, where $\\theta$ are the parameters of the model to be determined."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example 1: Classifying MNIST Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [MNIST](https://yann.lecun.com/exdb/mnist/) dataset consists of black-and-white images of handwritten digits, each normalized to fit within a 28x28 pixel bounding box. The images are anti-aliased, introducing grayscale levels for smoother edges. The dataset includes 60,000 training images and 10,000 testing images, making it a widely used benchmark for image classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Goal: Find a model to classify images of handwritten digits of the MNIST Dataset into their corresponding digit labels.`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import os\n",
    "import numpy as np\n",
    "import struct\n",
    "\n",
    "\n",
    "def load_mnist(path, kind='train'):\n",
    "    \"\"\"Load MNIST data from `path`\"\"\"\n",
    "    labels_path = os.path.join(path, f'{kind}-labels-idx1-ubyte.gz')\n",
    "    images_path = os.path.join(path, f'{kind}-images-idx3-ubyte.gz')\n",
    "\n",
    "    with gzip.open(labels_path, 'rb') as lbpath:\n",
    "        _, _ = struct.unpack('>II', lbpath.read(8))\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
    "\n",
    "    with gzip.open(images_path, 'rb') as imgpath:\n",
    "        _, num, rows, cols = struct.unpack('>IIII', imgpath.read(16))\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(num, rows*cols)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "mnist_path = '../supplemental_material/MNIST' \n",
    "X_train, y_train = load_mnist(mnist_path, kind='train')\n",
    "X_test, y_test = load_mnist(mnist_path, kind='t10k')\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, Test labels shape: {y_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify digits with or without circles using Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Define the digits with and without circles\n",
    "circle_digits = [0, 6, 8, 9]\n",
    "no_circle_digits = [1, 2, 3, 4, 5, 7]\n",
    "\n",
    "# Filter the training and test sets to include only the relevant digits\n",
    "train_mask = np.isin(y_train, circle_digits + no_circle_digits)\n",
    "X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "\n",
    "test_mask = np.isin(y_test, circle_digits + no_circle_digits)\n",
    "X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "\n",
    "y_train_binary = np.where(np.isin(y_train, circle_digits), 1, 0)\n",
    "y_test_binary = np.where(np.isin(y_test, circle_digits), 1, 0)\n",
    "\n",
    "\n",
    "\n",
    "# Normalize the data (important for gradient descent)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The formula for logistic regression is:\n",
    "\n",
    "$$\n",
    "P(y = 1 \\mid X) = \\sigma(w^T X + b) = \\frac{1}{1 + e^{-(w^T X + b)}}\n",
    "$$\n",
    "\n",
    "where:\n",
    "- $P(y = 1 \\mid X)$ is the probability that the output $y$ is 1 given the input features $X$.\n",
    "- $\\sigma(z) = \\frac{1}{1 + e^{-z}}$ is the sigmoid function.\n",
    "- $w$ is the vector of weights.\n",
    "- $X$ is the vector of input features.\n",
    "- $b$ is the bias term.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sigmoid function\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "# Initialize weights and bias\n",
    "def initialize_parameters(dim):\n",
    "    w = np.zeros((dim, 1))\n",
    "    b = 0\n",
    "    return w, b\n",
    "\n",
    "# Forward and backward propagation\n",
    "def propagate(w, b, X, Y):\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # Forward propagation\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    cost = -1/m * np.sum(Y * np.log(A) + (1 - Y) * np.log(1 - A))  # Compute cost\n",
    "\n",
    "    # Backward propagation\n",
    "    dw = 1/m * np.dot(X, (A - Y).T)\n",
    "    db = 1/m * np.sum(A - Y)\n",
    "    \n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grads, cost\n",
    "\n",
    "# Optimization using gradient descent\n",
    "def optimize(w, b, X, Y, num_iterations, learning_rate):\n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        # Calculate gradients and cost\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve gradients\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # Update parameters\n",
    "        w -= learning_rate * dw\n",
    "        b -= learning_rate * db\n",
    "        \n",
    "        # Record the cost every 100 iterations\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            print(f\"Cost after iteration {i}: {cost}\")\n",
    "    \n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return params, grads, costs\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make predictions, we use the learned weights \\( w \\) and bias \\( b \\) to compute the output \\( Z \\) and apply the sigmoid function to get the probability. We threshold the probability to determine the class label:\n",
    "\n",
    "$$\n",
    "\\text{predictions} = I(A > 0.5)\n",
    "$$\n",
    "\n",
    "where \\( I \\) is the indicator function that outputs 1 if true and 0 otherwise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict the labels\n",
    "def predict(w, b, X):\n",
    "    Z = np.dot(w.T, X) + b\n",
    "    A = sigmoid(Z)\n",
    "    predictions = (A > 0.5).astype(int)\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the data (required for our implementation)\n",
    "X_train = X_train.T\n",
    "X_test = X_test.T\n",
    "y_train_binary = y_train_binary.reshape(1, -1)\n",
    "y_test_binary = y_test_binary.reshape(1, -1)\n",
    "\n",
    "# Initialize parameters\n",
    "w, b = initialize_parameters(X_train.shape[0])\n",
    "\n",
    "# Train the model\n",
    "parameters, grads, costs = optimize(w, b, X_train, y_train_binary, num_iterations=2000, learning_rate=0.01)\n",
    "\n",
    "# Get the optimized parameters\n",
    "w = parameters[\"w\"]\n",
    "b = parameters[\"b\"]\n",
    "\n",
    "# Make predictions on the training and test sets\n",
    "y_pred_train = predict(w, b, X_train)\n",
    "y_pred_test = predict(w, b, X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "train_accuracy = 100 - np.mean(np.abs(y_pred_train - y_train_binary)) * 100\n",
    "test_accuracy = 100 - np.mean(np.abs(y_pred_test - y_test_binary)) * 100\n",
    "\n",
    "print(f\"Train accuracy: {train_accuracy}%\")\n",
    "print(f\"Test accuracy: {test_accuracy}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "y_test_flat = y_test.flatten()\n",
    "y_pred_test_flat = y_pred_test.flatten()\n",
    "\n",
    "cm = np.zeros((2, 10), dtype=int) \n",
    "\n",
    "for i, true_label in enumerate(y_test_flat):\n",
    "    predicted_binary = y_pred_test_flat[i]\n",
    "    if predicted_binary == 1:  \n",
    "        cm[1, true_label] += 1 # Predicted as a circle digit\n",
    "    else:  \n",
    "        cm[0, true_label] += 1  # Predicted as a no-circle digit\n",
    "\n",
    "# Plot the confusion matrix\n",
    "plt.figure(figsize=(12, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'], yticklabels=['No Circle', 'Circle'])\n",
    "plt.xlabel('Digit Label')\n",
    "plt.ylabel('Binary Classification')\n",
    "plt.title('Confusion Matrix for Binary Classification (Circle vs No-Circle) with Digit Labels')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from skimage import measure, morphology\n",
    "\n",
    "images = X_test.T.reshape(-1, 28, 28)  # Transpose and reshape to get (10000, 28, 28)\n",
    "\n",
    "\n",
    "# Define function to count white pixels inside circles for specific digits (0, 6, 8, 9)\n",
    "def count_inner_white_pixels(images, labels, circle_digits=[0, 6, 8, 9]):\n",
    "    inner_white_counts = np.zeros_like(labels, dtype=int)\n",
    "    \n",
    "    for i, (image, label) in enumerate(zip(images, labels)):\n",
    "        if label in circle_digits:\n",
    "            # Threshold image for white (background and inner circles) and black (digit contours)\n",
    "            binary_white = image >= 200  # White areas\n",
    "            binary_black = image <= 50   # Black areas (digit shape)\n",
    "\n",
    "            # Identify connected components in the binary white mask\n",
    "            labeled_white = measure.label(binary_white, connectivity=2)\n",
    "            regions = measure.regionprops(labeled_white)\n",
    "\n",
    "            # Filter regions based on size and position within the digit bounds\n",
    "            inner_white_pixels = 0\n",
    "            for region in regions:\n",
    "                # Check if the region is within the digit bounds and not part of the outer background\n",
    "                if region.bbox[0] > 3 and region.bbox[1] > 3 and region.bbox[2] < 25 and region.bbox[3] < 25:\n",
    "                    # Assuming this segment is an \"inner circle\" if inside digit bounds\n",
    "                    inner_white_pixels += region.area\n",
    "\n",
    "            inner_white_counts[i] = inner_white_pixels\n",
    "\n",
    "    return inner_white_counts\n",
    "\n",
    "# Count white pixels in inner circles for the circle digits\n",
    "inner_white_pixel_counts = count_inner_white_pixels(images, y_test_flat)\n",
    "\n",
    "# Plot histograms for inner white pixels only in circle digits (0, 6, 8, 9)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "\n",
    "for idx, digit in enumerate(circle_digits):\n",
    "    digit_inner_white_pixels = inner_white_pixel_counts[y_test_flat == digit]\n",
    "    \n",
    "    axes[idx].hist(digit_inner_white_pixels, bins=20, color='lightblue', edgecolor='black')\n",
    "    axes[idx].set_title(f\"Digit {digit}\")\n",
    "    axes[idx].set_xlabel(\"Inner White Pixels\")\n",
    "\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "fig.suptitle(\"Histogram of Inner White Pixels in Circle Digits (0, 6, 8, 9)\", fontsize=16)\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images = X_test.T.reshape(-1, 28, 28)  # Transpose and reshape to get (10000, 28, 28)\n",
    "\n",
    "\n",
    "def count_white_black_pixels(images):\n",
    "    white_counts = np.sum(images >= 0, axis=(1, 2))  # White pixel threshold at 200+\n",
    "    black_counts = np.sum(images <= 0, axis=(1, 2))   # Black pixel threshold at 50-\n",
    "    return white_counts, black_counts\n",
    "\n",
    "\n",
    "# Count white and black pixels in the reshaped images\n",
    "white_pixel_counts, black_pixel_counts = count_white_black_pixels(images)\n",
    "\n",
    "# Plot histogram for white pixels in circle digits (0, 6, 8, 9)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "\n",
    "for idx, digit in enumerate(circle_digits):\n",
    "    digit_white_pixels = white_pixel_counts[y_test_flat == digit]\n",
    "    axes[idx].hist(digit_white_pixels, bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[idx].set_title(f\"Digit {digit}\")\n",
    "    axes[idx].set_xlabel(\"White Pixels\")\n",
    "\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "fig.suptitle(\"Histogram of White Pixels in Circle Digits (0, 6, 8, 9)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming we have y_test_flat and y_pred_test_flat ready in the dataset\n",
    "# For the sake of this example, we will simulate a subset of images for digits 0-9 \n",
    "# with each image being a 28x28 grayscale image and labels\n",
    "\n",
    "# Simulated dataset for illustration\n",
    "np.random.seed(0)\n",
    "sample_size = 100  # Sample size for each digit\n",
    "image_size = 28 * 28  # MNIST images are 28x28 pixels\n",
    "\n",
    "# Generating random grayscale images as placeholders (values between 0 and 255)\n",
    "# In an actual scenario, we would have real MNIST image data\n",
    "images = np.random.randint(0, 256, size=(10 * sample_size, 28, 28))\n",
    "labels = np.repeat(range(10), sample_size)\n",
    "\n",
    "# Function to count white and black pixels\n",
    "def count_white_black_pixels(images):\n",
    "    white_counts = np.sum(images >= 200, axis=(1, 2))  # White pixel threshold at 200+\n",
    "    black_counts = np.sum(images <= 50, axis=(1, 2))   # Black pixel threshold at 50-\n",
    "    return white_counts, black_counts\n",
    "\n",
    "# Count white and black pixels for all images\n",
    "white_pixel_counts, black_pixel_counts = count_white_black_pixels(images)\n",
    "\n",
    "# Plot histogram for white pixels in circle digits (0, 6, 8, 9)\n",
    "fig, axes = plt.subplots(1, 4, figsize=(20, 5), sharey=True)\n",
    "\n",
    "# Plotting separate histograms for each circle digit (0, 6, 8, 9) for white pixels\n",
    "for idx, digit in enumerate(circle_digits):\n",
    "    digit_white_pixels = white_pixel_counts[labels == digit]\n",
    "    \n",
    "    axes[idx].hist(digit_white_pixels, bins=20, color='skyblue', edgecolor='black')\n",
    "    axes[idx].set_title(f\"Digit {digit}\")\n",
    "    axes[idx].set_xlabel(\"White Pixels\")\n",
    "\n",
    "axes[0].set_ylabel(\"Frequency\")\n",
    "fig.suptitle(\"Histogram of White Pixels in Circle Digits (0, 6, 8, 9)\", fontsize=16)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Reshape the data back to the format expected by sklearn\n",
    "X_train_sklearn = X_train.T  # Shape: (num_samples, num_features)\n",
    "X_test_sklearn = X_test.T    # Shape: (num_samples, num_features)\n",
    "y_train_sklearn = y_train.flatten()  # Shape: (num_samples,)\n",
    "y_test_sklearn = y_test.flatten()    # Shape: (num_samples,)\n",
    "\n",
    "# Train the logistic regression model using sklearn\n",
    "model = LogisticRegression(solver='lbfgs', max_iter=2000, n_jobs=-1)\n",
    "model.fit(X_train_sklearn, y_train_sklearn)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = model.predict(X_test_sklearn)\n",
    "\n",
    "# Evaluate the model\n",
    "accuracy = accuracy_score(y_test_sklearn, y_pred)\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify digits using MultiClass Logistic regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In multiclass logistic regression, we extend the binary case to handle multiple classes. We use learned weights \\( W \\) and bias \\( b \\) to compute the output \\( Z \\) for each class. The softmax function is then applied to obtain the probabilities for each class:\n",
    "\n",
    "$$\n",
    "P(y = k | \\mathbf{x}) = \\frac{e^{Z_k}}{\\sum_{j=1}^{K} e^{Z_j}}\n",
    "$$\n",
    "\n",
    "where $Z_k$ is the output for class $k$, $K$ is the total number of classes, and $\\mathbf{x}$ is the input feature vector. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted class label is determined by:\n",
    "\n",
    "$$\n",
    "\\text{predictions} = \\text{argmax}(P(y = k | \\mathbf{x}))\n",
    "$$\n",
    "\n",
    "where $\\text{argmax}$ selects the class with the highest probability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model = LogisticRegression(solver='lbfgs', max_iter=2000, random_state=42)\n",
    "sklearn_model.fit(X_train.T, y_train.flatten())  # Remember to transpose X_train for scikit-learn compatibility\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred_test_sklearn = sklearn_model.predict(X_test.T)\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy_sklearn = accuracy_score(y_test.flatten(), y_pred_test_sklearn)\n",
    "print(f\"Test accuracy: {test_accuracy_sklearn * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classify digits with or witout circles using Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The goal of SVM is to find a hyperplane that maximizes the margin between two classes. Given a dataset of $N$ samples $(x_i, y_i)$ where $x_i \\in \\mathbb{R}^d$ is the feature vector and $y_i \\in \\{-1, 1\\}$ is the class label, we aim to solve the following optimization problem:\n",
    "\n",
    "$$\n",
    "\\text{minimize } J(w, b) = \\frac{1}{2} \\|w\\|^2 + C \\sum_{i=1}^{N} \\max\\left(0, 1 - y_i (w \\cdot x_i + b)\\right)\n",
    "$$\n",
    "\n",
    "### Definitions\n",
    "\n",
    "- $w$: Weight vector (coefficients of the features).\n",
    "- $b$: Bias term (intercept of the hyperplane).\n",
    "- $C$: Regularization parameter that controls the trade-off between maximizing the margin and minimizing the classification error.\n",
    "- $N$: Number of training samples.\n",
    "- $d$: Number of features in each sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "mnist_path = '../supplemental_material/MNIST' \n",
    "X_train, y_train = load_mnist(mnist_path, kind='train')\n",
    "X_test, y_test = load_mnist(mnist_path, kind='t10k')\n",
    "\n",
    "print(f\"Training set shape: {X_train.shape}, Training labels shape: {y_train.shape}\")\n",
    "print(f\"Test set shape: {X_test.shape}, Test labels shape: {y_test.shape}\")\n",
    "\n",
    "# Define the digits with and without circles\n",
    "circle_digits = [0, 6, 8, 9]\n",
    "no_circle_digits = [1, 2, 3, 4, 5, 7]\n",
    "\n",
    "# Filter the training and test sets to include only the relevant digits\n",
    "train_mask = np.isin(y_train, circle_digits + no_circle_digits)\n",
    "X_train, y_train = X_train[train_mask], y_train[train_mask]\n",
    "\n",
    "test_mask = np.isin(y_test, circle_digits + no_circle_digits)\n",
    "X_test, y_test = X_test[test_mask], y_test[test_mask]\n",
    "\n",
    "# Relabel the data: 1 for circle digits, 0 for no-circle digits\n",
    "y_train = np.where(np.isin(y_train, circle_digits), 1, 0)\n",
    "y_test = np.where(np.isin(y_test, circle_digits), 1, 0)\n",
    "\n",
    "# Normalize the data (important for gradient descent)\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM:\n",
    "    def __init__(self, learning_rate=0.001, lambda_param=0.01, num_iterations=1000):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.lambda_param = lambda_param\n",
    "        self.num_iterations = num_iterations\n",
    "        self.w = None\n",
    "        self.b = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape\n",
    "        y = np.where(y <= 0, -1, 1)  # Convert labels to -1 and 1 for SVM\n",
    "\n",
    "        # Initialize weights and bias\n",
    "        self.w = np.zeros(num_features)\n",
    "        self.b = 0\n",
    "\n",
    "        # Gradient descent\n",
    "        for _ in range(self.num_iterations):\n",
    "            for idx, x_i in enumerate(X):\n",
    "                condition = y[idx] * (np.dot(x_i, self.w) - self.b) >= 1\n",
    "                if condition:\n",
    "                    # If the point is correctly classified and outside the margin\n",
    "                    dw = 2 * self.lambda_param * self.w\n",
    "                    db = 0\n",
    "                else:\n",
    "                    # If the point is inside the margin or misclassified\n",
    "                    dw = 2 * self.lambda_param * self.w - np.dot(x_i, y[idx])\n",
    "                    db = y[idx]\n",
    "\n",
    "                # Update weights and bias\n",
    "                self.w -= self.learning_rate * dw\n",
    "                self.b -= self.learning_rate * db\n",
    "\n",
    "    def predict(self, X):\n",
    "        approx = np.dot(X, self.w) - self.b\n",
    "        return np.sign(approx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize and train the SVM model\n",
    "svm = SVM(learning_rate=0.001, lambda_param=0.01, num_iterations=1000)\n",
    "svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test = svm.predict(X_test)\n",
    "\n",
    "# Convert predictions back to 0 and 1\n",
    "y_pred_test = np.where(y_pred_test == -1, 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model\n",
    "test_accuracy = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test accuracy (SVM from scratch): {test_accuracy * 100:.2f}%\")\n",
    "\n",
    "# Confusion matrix and classification report\n",
    "print(\"Confusion Matrix for Test Set (SVM from scratch):\")\n",
    "conf_matrix = confusion_matrix(y_test, y_pred_test)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.title('Confusion Matrix (SVM from scratch)')\n",
    "plt.show()\n",
    "\n",
    "print(\"Classification Report (SVM from scratch):\")\n",
    "print(classification_report(y_test, y_pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize and train the scikit-learn SVM model\n",
    "sklearn_svm = SVC(kernel='linear')\n",
    "sklearn_svm.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred_test_sklearn = sklearn_svm.predict(X_test)\n",
    "\n",
    "# Evaluate the scikit-learn model\n",
    "test_accuracy_sklearn = accuracy_score(y_test, y_pred_test_sklearn)\n",
    "print(f\"Test accuracy (scikit-learn SVM): {test_accuracy_sklearn * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digits classification using Multi Layer Perceptron\n",
    "A Multi-Layer Perceptron (MLP) is a feedforward neural network with multiple layers of neurons that utilize weighted connections and activation functions to learn complex mappings from inputs to outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the MLP can be represented as:\n",
    "\n",
    "$$\n",
    "Z^{(1)} = W^{(1)} X + b^{(1)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{(1)} = \\text{ReLU}(Z^{(1)})\n",
    "$$\n",
    "\n",
    "$$\n",
    "Z^{(2)} = W^{(2)} A^{(1)} + b^{(2)}\n",
    "$$\n",
    "\n",
    "$$\n",
    "A^{(2)} = \\sigma(Z^{(2)})\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "- $W^{(1)}, W^{(2)}$: Weight matrices for the first and second layers.\n",
    "- $b^{(1)}, b^{(2)}$: Bias vectors for the first and second layers.\n",
    "- $X$: Input features.\n",
    "- $A^{(1)}, A^{(2)}$: Activations for the first and second layers.\n",
    "- $\\text{ReLU}$: Rectified Linear Unit activation function.\n",
    "- $\\sigma$: Sigmoid activation function for binary classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Define the MLP model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(MLP, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.sigmoid(out)\n",
    "        return out\n",
    "\n",
    "# Prepare the data\n",
    "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
    "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Initialize the model, loss function, and optimizer\n",
    "input_size = X_train.shape[1]  # 784 for MNIST\n",
    "hidden_size = 128\n",
    "output_size = 1  # Binary classification\n",
    "\n",
    "model = MLP(input_size, hidden_size, output_size)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "\n",
    "# Save the model weights\n",
    "save_path = os.path.join(\"data\", \"weights\", \"MLP\")\n",
    "os.makedirs(save_path, exist_ok=True)  \n",
    "torch.save(model.state_dict(), os.path.join(save_path, \"MNIST.pth\"))\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    y_pred_train = model(X_train_tensor).round()\n",
    "    y_pred_test = model(X_test_tensor).round()\n",
    "\n",
    "train_accuracy = (y_pred_train.eq(y_train_tensor).sum() / y_train_tensor.shape[0]).item()\n",
    "test_accuracy = (y_pred_test.eq(y_test_tensor).sum() / y_test_tensor.shape[0]).item()\n",
    "\n",
    "print(f'Train Accuracy: {train_accuracy * 100:.2f}%')\n",
    "print(f'Test Accuracy: {test_accuracy * 100:.2f}%')\n",
    "\n",
    "# Confusion Matrix\n",
    "y_pred_test_np = y_pred_test.numpy().flatten()\n",
    "y_test_np = y_test_tensor.numpy().flatten()\n",
    "\n",
    "cm = confusion_matrix(y_test_np, y_pred_test_np)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"No Circle\", \"Circle\"])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Confusion Matrix for MLP (Circle vs No-Circle)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Digit Classification with LeNet\n",
    "LeNet is a convolutional neural network (CNN) architecture that employs convolutional and subsampling layers to extract features from images, followed by fully connected layers for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of the LeNet Architecture can be represented as:\n",
    "\n",
    "1. **Convolution and Activation:**\n",
    "   $$\n",
    "   Z^{(1)} = W^{(1)} * X + b^{(1)}\n",
    "   $$\n",
    "   $$\n",
    "   A^{(1)} = \\text{tanh}(Z^{(1)})\n",
    "   $$\n",
    "\n",
    "2. **Subsampling (Pooling):**\n",
    "   $$\n",
    "   A^{(2)} = \\text{AvgPool}(A^{(1)})\n",
    "   $$\n",
    "\n",
    "3. **Second Convolution and Activation:**\n",
    "   $$\n",
    "   Z^{(3)} = W^{(3)} * A^{(2)} + b^{(3)}\n",
    "   $$\n",
    "   $$\n",
    "   A^{(3)} = \\text{tanh}(Z^{(3)})\n",
    "   $$\n",
    "\n",
    "4. **Second Subsampling (Pooling):**\n",
    "   $$\n",
    "   A^{(4)} = \\text{AvgPool}(A^{(3)})\n",
    "   $$\n",
    "\n",
    "5. **Flattening:**\n",
    "   $$\n",
    "   A^{(5)} = \\text{Flatten}(A^{(4)})\n",
    "   $$\n",
    "\n",
    "6. **Fully Connected Layer:**\n",
    "   $$\n",
    "   Z^{(6)} = W^{(6)} A^{(5)} + b^{(6)}\n",
    "   $$\n",
    "   $$\n",
    "   A^{(6)} = \\sigma(Z^{(6)})\n",
    "   $$\n",
    "\n",
    "Where:\n",
    "- $W^{(l)}$: Weight matrices for each layer.\n",
    "- $b^{(l)}$: Bias vectors for each layer.\n",
    "- $X$: Input image.\n",
    "- $\\text{tanh}$: Hyperbolic tangent activation function.\n",
    "- $\\text{AvgPool}$: Average pooling operation.\n",
    "- $\\sigma$: Sigmoid activation function for output classification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pooling is a downsampling operation in CNNs that reduces feature map dimensions by summarizing local regions, commonly using max or average function.  \n",
    "Flattening transforms a multi-dimensional tensor into a one-dimensional vector, preparing feature maps for input into fully connected layers in neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "# Define LeNet-5 architecture\n",
    "class LeNet5(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet5, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 2)  # Output 2 classes: circle and no-circle\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.conv1(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = torch.relu(self.conv2(x))\n",
    "        x = torch.max_pool2d(x, kernel_size=2, stride=2)\n",
    "        x = x.view(-1, 16*5*5)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "# Define the digits with and without circles\n",
    "circle_digits = [0, 6, 8, 9]\n",
    "no_circle_digits = [1, 2, 3, 4, 5, 7]\n",
    "\n",
    "# Transform the data to tensor and normalize\n",
    "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
    "\n",
    "# Function to filter and label the data\n",
    "def filter_and_label_data(X, y, circle_digits, no_circle_digits):\n",
    "    mask = np.isin(y, circle_digits + no_circle_digits)\n",
    "    X_filtered, y_filtered = X[mask], y[mask]\n",
    "    y_filtered = np.where(np.isin(y_filtered, circle_digits), 1, 0)\n",
    "    return X_filtered, y_filtered\n",
    "\n",
    "# Load the dataset\n",
    "X_train, y_train = load_mnist(mnist_path, kind='train')\n",
    "X_test, y_test = load_mnist(mnist_path, kind='t10k')\n",
    "\n",
    "X_train, y_train = filter_and_label_data(X_train, y_train, circle_digits, no_circle_digits)\n",
    "X_test, y_test = filter_and_label_data(X_test, y_test, circle_digits, no_circle_digits)\n",
    "\n",
    "# Reshape data to (n_samples, 1, 28, 28) for PyTorch Conv2D\n",
    "X_train = X_train.reshape(-1, 1, 28, 28).astype(np.float32)\n",
    "X_test = X_test.reshape(-1, 1, 28, 28).astype(np.float32)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "X_train_tensor = torch.tensor(X_train)\n",
    "y_train_tensor = torch.tensor(y_train).long()\n",
    "X_test_tensor = torch.tensor(X_test)\n",
    "y_test_tensor = torch.tensor(y_test).long()\n",
    "\n",
    "# Create datasets and data loaders\n",
    "train_dataset = TensorDataset(X_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(X_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Instantiate the model, define the loss function and the optimizer\n",
    "model = LeNet5()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for X_batch, y_batch in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "\n",
    "save_path = os.path.join(\"data\", \"weights\", \"Lenet5\")\n",
    "os.makedirs(save_path, exist_ok=True)  \n",
    "torch.save(model.state_dict(), os.path.join(save_path, \"MNIST.pth\"))\n",
    "\n",
    "# Evaluation\n",
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += y_batch.size(0)\n",
    "        correct += (predicted == y_batch).sum().item()\n",
    "\n",
    "print(f'Accuracy: {100 * correct / total}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
